# ğŸ“° Multi-Level News Classifier â€” TF-IDF vs BERT

This project compares two text classification approaches on the **AG News dataset**:
1. **Baseline:** TF-IDF + Logistic Regression  
2. **Advanced:** Fine-tuned BERT (`bert-base-uncased`)  

---

## ğŸ“Š Dataset
The [AG News dataset](https://www.kaggle.com/datasets/amananandrai/ag-news-classification-dataset) contains **4 categories**:
- World  
- Sports  
- Business  
- Sci/Tech  

Each record has a *Title* and *Description* column.

---

## ğŸ“ Project Structure

```
news-classifier/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ train.csv
â”‚   â”œâ”€â”€ test.csv
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_EDA.ipynb
â”‚   â”œâ”€â”€ 01_EDA.html
â”‚   â”œâ”€â”€ 02_baseline_tfidf.ipynb
â”‚   â”œâ”€â”€ 02_baseline_tfidf.html
â”‚   â”œâ”€â”€ 03_bert_finetune.ipynb
â”‚
â””â”€â”€ reports/
    â”œâ”€â”€ tf_idf_baseline_confusion_matrix.png
    â”œâ”€â”€ bert_confusion_matrix.png
```

---


## ğŸ“˜ Notebooks (clickable)

| Notebook | Description |
|-----------|--------------|
| [01_EDA.html (nbviewer)](https://nbviewer.org/github/drnursultan/news-classifier-tfidf-bert/blob/main/notebooks/01_EDA.html) | Exploratory Data Analysis |
| [02_Baseline_TFIDF.html (nbviewer)](https://nbviewer.org/github/drnursultan/news-classifier-tfidf-bert/blob/main/notebooks/02_baseline_tfidf.html) | TF-IDF Baseline Model |
| [03_bert_finetune_clean.html](notebooks/03_bert_finetune_clean.html) | Fine-Tuned BERT Model |

---

## ğŸ§  Model Comparison

| Metric | TF-IDF + Logistic Regression | BERT (Fine-Tuned) |
|:-------|:-----------------------------:|:-----------------:|
| Accuracy | **0.92** | **0.95** |
| Precision (avg) | 0.92 | 0.95 |
| Recall (avg) | 0.92 | 0.95 |
| F1-Score (avg) | 0.92 | 0.95 |
| Training Epochs | â€“ | 2 |
| Training Loss (last) | â€“ | 0.1120 |
| Validation Loss (last) | â€“ | 0.1886 |

---

## ğŸ“ˆ Confusion Matrices

| TF-IDF | BERT |
|:------:|:----:|
| <img src="reports/tf_idf_baseline_confusion_matrix.png" width="400"/> | <img src="reports/bert_confusion_matrix.png" width="400"/> |

---

## ğŸ§© Evaluation Reports

### BERT Report
```
          precision    recall  f1-score   support

   World       0.96      0.96      0.96      1900
  Sports       0.99      0.99      0.99      1900
Business       0.93      0.91      0.92      1900
Sci/Tech       0.91      0.94      0.92      1900

accuracy                           0.95      7600
```

```
macro avg          0.95      0.95      0.95      7600
weighted avg       0.95      0.95      0.95      7600
```
### TF-IDF Report

```
          precision    recall  f1-score   support

       1       0.94      0.91      0.92      1900
       2       0.95      0.98      0.97      1900
       3       0.90      0.88      0.89      1900
       4       0.89      0.91      0.90      1900

accuracy                           0.92      7600
```

```
macro avg          0.92      0.92      0.92      7600
weighted avg       0.92      0.92      0.92      7600
````
---

## ğŸ§  Summary

| Model | Strength | Weakness |
|--------|-----------|-----------|
| **TF-IDF** | Fast, interpretable | Ignores word context |
| **BERT** | Understands meaning, higher accuracy | Slow to train, needs GPU |

---

## ğŸš€ How to Run

1. Open the notebook in **Google Colab**  
2. Mount your Google Drive and load the dataset  
3. Run `03_bert_finetune.ipynb` (enable GPU)  

To compare, open the HTML reports above for metrics and visual results.

---

## ğŸ·ï¸ Author
Developed by **Nursultan Azhimuratov**  
University of Wisconsinâ€“Madison | Data Science & NLP Projects
